{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57b8557",
   "metadata": {},
   "source": [
    "\n",
    "# JPX Quant Interview — End‑to‑End Modelling Workflow\n",
    "\n",
    "**Author:** _Your Name Here_  \n",
    "**Date:** _YYYY‑MM‑DD_\n",
    "\n",
    "This notebook is structured for a 6‑hour interview workflow. It contains two deliverables:\n",
    "1. **Written report** (fill the Markdown sections as you progress).\n",
    "2. **Modelling code** (no lookahead bias, start simple then get more complex, interpretable, evaluated with sensible metrics).\n",
    "\n",
    "> Tip: Run cells **top‑to‑bottom**. Where data paths differ from your environment, modify the file paths in the **Data Loading** cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd3788",
   "metadata": {},
   "source": [
    "\n",
    "## Part I — Written Report (Template)\n",
    "\n",
    "### 1. Problem definition\n",
    "- **Objective:** Predict next‑period target return (`Target`) per security / rank securities for daily portfolio formation.\n",
    "- **Universe & horizon:** JPX (Tokyo Stock Exchange) equities, daily horizon.\n",
    "- **Constraints:** No lookahead bias; features observable **at or before** prediction time.\n",
    "\n",
    "### 2. Data audit (concrete answers)\n",
    "- **Files & shapes:** List files loaded and their row/column counts.\n",
    "- **Date coverage:** First/last date per file; intersection across files.\n",
    "- **Key columns:** `Date`, `SecuritiesCode`, `Open, High, Low, Close, Volume`, other fundamentals.\n",
    "- **Missingness:** % missing per column; how imputed.\n",
    "- **Types:** Any `object` dtypes in numeric columns? How fixed.\n",
    "- **Outliers:** How detected; treatment (e.g., per‑group z‑score + clip).\n",
    "\n",
    "> _Answer block:_ Fill in bullet‑point facts (numbers, counts, %).\n",
    "\n",
    "### 3. Feature logic (interpretability first)\n",
    "- **Level 0 features:** Prices/Volume, ranges, rolling returns/volatility (no leakage).\n",
    "- **Normalization:** Per‑security standardization using **training‑only** stats.\n",
    "- **Leakage guards:** Rolling windows with `min_periods` and `.shift(1)` before aggregations.\n",
    "\n",
    "### 4. Modelling plan (simple → complex)\n",
    "- **Baseline‑0:** Naïve (predict 0 or previous day’s `Target`).\n",
    "- **Baseline‑1:** Cross‑sectional OLS per day or pooled OLS with fixed effects.\n",
    "- **Regularized:** Ridge/Lasso (better stability).\n",
    "- **Nonlinear (still interpretable enough):** Gradient boosting with monotonic reasoning; use SHAP/feature importances for explanation.\n",
    "\n",
    "### 5. Evaluation\n",
    "- **Point metrics:** MAE, RMSE, R² (pooled and by date).\n",
    "- **Cross‑sectional rank skill:** Daily Spearman IC, mean IC, ICIR.\n",
    "- **Portfolio utility:** Top‑K long (and long‑short) daily backtest using the provided `Target` as realized return.\n",
    "- **Data splits:** Time‑based Train / Validation / Test (e.g., Train ≤ 2021‑12‑31, Val 2022‑Q1, Test ≥ 2022‑04‑01).\n",
    "\n",
    "### 6. Results (right/wrong)\n",
    "- **Table:** Baseline vs models with metrics (MAE, R², mean IC, ICIR, Top‑K return).\n",
    "- **Observations:** What worked, what didn’t—and why.\n",
    "- **Sanity checks:** Leakage tests, stability across securities and time, turnover.\n",
    "\n",
    "### 7. Conclusion & next steps\n",
    "- **Pick a model:** Rationale (interpretability vs performance).\n",
    "- **Production considerations:** Retraining cadence, failure modes, monitoring (IC drift), and risk controls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6b9d33",
   "metadata": {},
   "source": [
    "## 0. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1178312d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Please set `prices_path` to an existing CSV.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m         prices\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mread_csv(p))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease set `prices_path` to an existing CSV.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m stock_prices \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(prices, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(stock_prices\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Please set `prices_path` to an existing CSV."
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Update these paths to your environment\n",
    "# For Kaggle JPX competition layout, you might set input_dir accordingly.\n",
    "input_dir = Path('../input/jpx-tokyo-stock-exchange-prediction/train_files')\n",
    "\n",
    "# If you have a single merged prices file, point to it here:\n",
    "# Example placeholder: Path('/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv')\n",
    "prices_path = input_dir / 'stock_prices.csv'\n",
    "\n",
    "# Optional: load secondary/supplemental and concat if present\n",
    "paths = [prices_path]\n",
    "prices = []\n",
    "for p in paths:\n",
    "    if p.exists():\n",
    "        prices.append(pd.read_csv(p))\n",
    "if len(prices) == 0:\n",
    "    raise FileNotFoundError(\"Please set `prices_path` to an existing CSV.\")\n",
    "stock_prices = pd.concat(prices, ignore_index=True)\n",
    "\n",
    "print(stock_prices.shape)\n",
    "stock_prices.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16a15ee",
   "metadata": {},
   "source": [
    "## 1. Data Audit Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def audit_dataframe(df: pd.DataFrame, date_col='Date', code_col='SecuritiesCode'):\n",
    "    info = {}\n",
    "    info['shape'] = df.shape\n",
    "    info['columns'] = list(df.columns)\n",
    "    info['dtypes'] = df.dtypes.astype(str).to_dict()\n",
    "    # Missingness\n",
    "    miss = df.isna().mean().sort_values(ascending=False)\n",
    "    info['missing_ratio'] = miss[miss>0].to_dict()\n",
    "    # Date coverage\n",
    "    if date_col in df.columns:\n",
    "        try:\n",
    "            d = pd.to_datetime(df[date_col], errors='coerce')\n",
    "            info['date_min'] = str(d.min())\n",
    "            info['date_max'] = str(d.max())\n",
    "        except Exception as e:\n",
    "            info['date_parse_error'] = str(e)\n",
    "    # Security count\n",
    "    if code_col in df.columns:\n",
    "        info['n_securities'] = df[code_col].nunique()\n",
    "    return info\n",
    "\n",
    "audit = audit_dataframe(stock_prices)\n",
    "audit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f068e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find object-typed columns\n",
    "obj_cols = stock_prices.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Object dtype columns:\", obj_cols)\n",
    "stock_prices[obj_cols].head() if obj_cols else \"No object columns\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbccb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detect problematic rows for numeric columns of interest\n",
    "num_cols_guess = ['Open','High','Low','Close','Volume']\n",
    "num_cols = [c for c in num_cols_guess if c in stock_prices.columns]\n",
    "\n",
    "bad_map = {}\n",
    "for c in num_cols:\n",
    "    bad_mask = pd.to_numeric(stock_prices[c], errors='coerce').isna()\n",
    "    n_bad = bad_mask.sum()\n",
    "    if n_bad > 0:\n",
    "        bad_map[c] = int(n_bad)\n",
    "bad_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show actual offending values per numeric column (sample)\n",
    "offending_samples = {}\n",
    "for c in num_cols:\n",
    "    mask = pd.to_numeric(stock_prices[c], errors='coerce').isna()\n",
    "    if mask.any():\n",
    "        offending_samples[c] = stock_prices.loc[mask, c].head(10).tolist()\n",
    "offending_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e41db9a",
   "metadata": {},
   "source": [
    "## 2. Preprocessing (No Lookahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b76ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import stats\n",
    "\n",
    "df = stock_prices.copy()\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "# Minimal cleaning\n",
    "for c in ['ExpectedDividend','Target']:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "        df[c] = df[c].fillna(0.0)\n",
    "\n",
    "if 'SupervisionFlag' in df.columns:\n",
    "    df['SupervisionFlag'] = df['SupervisionFlag'].astype(int)\n",
    "\n",
    "# ffill/bfill for O/H/L/C per security ordered by Date\n",
    "price_cols = [c for c in ['Open','High','Low','Close'] if c in df.columns]\n",
    "df = df.sort_values(['SecuritiesCode','Date'])\n",
    "df[price_cols] = df.groupby('SecuritiesCode')[price_cols].apply(lambda g: g.ffill().bfill())\n",
    "\n",
    "# Feature engineering (leakage‑safe)\n",
    "# Use only info up to t (or t-1) to predict Target at t+1 (if Target is next-day return)\n",
    "# Here we assume Kaggle's `Target` is a realized future return aligned with date row.\n",
    "# To be safe, shift rolling features by 1 day.\n",
    "def add_features(g):\n",
    "    g = g.copy()\n",
    "    if 'Close' in g.columns and 'Open' in g.columns:\n",
    "        g['Daily_Range'] = g['Close'] - g['Open']\n",
    "    if 'High' in g.columns and 'Low' in g.columns:\n",
    "        g['Mid'] = 0.5*(g['High'] + g['Low'])\n",
    "    # Rolling returns/volatility (shifted to avoid leakage)\n",
    "    if 'Close' in g.columns:\n",
    "        g['ret_1d'] = g['Close'].pct_change(1)\n",
    "        g['ret_5d'] = g['Close'].pct_change(5)\n",
    "        g['vol_5d'] = g['ret_1d'].rolling(5, min_periods=4).std()\n",
    "        # Shift all derived features by 1 day so only past info is used\n",
    "        for c in ['ret_1d','ret_5d','vol_5d']:\n",
    "            g[c] = g[c].shift(1)\n",
    "    return g\n",
    "\n",
    "df = df.groupby('SecuritiesCode', group_keys=False).apply(add_features)\n",
    "\n",
    "# Per‑security standardization using TRAIN‑ONLY statistics will be handled below during splitting.\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd102b",
   "metadata": {},
   "source": [
    "## 3. Time‑Based Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose time cutoffs (adjust to your dataset coverage)\n",
    "train_end = pd.Timestamp('2021-12-31')\n",
    "val_end   = pd.Timestamp('2022-03-31')\n",
    "# Test starts at 2022-04-01+\n",
    "\n",
    "mask_train = df['Date'] <= train_end\n",
    "mask_val   = (df['Date'] > train_end) & (df['Date'] <= val_end)\n",
    "mask_test  = df['Date'] > val_end\n",
    "\n",
    "cols_keep = ['Date','SecuritiesCode','Target'] + [c for c in df.columns if c not in ['Date','SecuritiesCode','Target']]\n",
    "df = df[cols_keep]\n",
    "\n",
    "train = df.loc[mask_train].copy()\n",
    "val   = df.loc[mask_val].copy()\n",
    "test  = df.loc[mask_test].copy()\n",
    "\n",
    "train.shape, val.shape, test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5baac64",
   "metadata": {},
   "source": [
    "## 4. Train‑Only Scaling per Security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c11c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feat_cols = [c for c in df.columns if c not in ['Date','SecuritiesCode','Target']]\n",
    "def fit_group_stats(g):\n",
    "    return pd.Series({'mean': g.mean(), 'std': g.std(ddof=0)})\n",
    "\n",
    "# Fit stats on TRAIN only per security and feature\n",
    "stats_map = {}\n",
    "for sec, g in train.groupby('SecuritiesCode'):\n",
    "    s = {}\n",
    "    for c in feat_cols:\n",
    "        x = g[c]\n",
    "        m, sd = x.mean(), x.std(ddof=0)\n",
    "        s[c] = (m, sd if sd>0 else 1.0)\n",
    "    stats_map[sec] = s\n",
    "\n",
    "def apply_scale(frame):\n",
    "    frame = frame.copy()\n",
    "    for sec, g in frame.groupby('SecuritiesCode'):\n",
    "        s = stats_map.get(sec, None)\n",
    "        if s is None: \n",
    "            continue\n",
    "        for c in feat_cols:\n",
    "            if c in g.columns:\n",
    "                m, sd = s[c]\n",
    "                frame.loc[g.index, c] = (g[c] - m) / sd\n",
    "    # Clip extreme z-scores\n",
    "    frame[feat_cols] = frame[feat_cols].clip(-5, 5)\n",
    "    return frame\n",
    "\n",
    "train_s = apply_scale(train)\n",
    "val_s   = apply_scale(val)\n",
    "test_s  = apply_scale(test)\n",
    "\n",
    "train_s.shape, val_s.shape, test_s.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a447810",
   "metadata": {},
   "source": [
    "## 5. Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def daily_ic(df_like):\n",
    "    # Spearman IC by date (prediction vs realized Target)\n",
    "    out = []\n",
    "    for d, g in df_like.groupby('Date'):\n",
    "        if g['pred'].nunique() < 2 or g['Target'].nunique() < 2:\n",
    "            continue\n",
    "        ic = spearmanr(g['pred'], g['Target']).correlation\n",
    "        out.append(ic)\n",
    "    return pd.Series(out).dropna()\n",
    "\n",
    "def topk_backtest(df_like, k=200):\n",
    "    # Long top-k by predicted score each day; realized PnL = mean(Target of selected)\n",
    "    rets = []\n",
    "    for d, g in df_like.groupby('Date'):\n",
    "        g2 = g.sort_values('pred', ascending=False).head(k)\n",
    "        if len(g2) > 0:\n",
    "            rets.append(g2['Target'].mean())\n",
    "    if len(rets) == 0:\n",
    "        return np.nan, np.nan\n",
    "    arr = np.array(rets)\n",
    "    mean = arr.mean()\n",
    "    std = arr.std(ddof=1) if arr.std(ddof=1) > 0 else np.nan\n",
    "    icir = np.nan\n",
    "    return mean, std\n",
    "\n",
    "# Baseline-0: predict 0\n",
    "for split_name, frame in [('VAL', val_s), ('TEST', test_s)]:\n",
    "    tmp = frame[['Date','SecuritiesCode','Target']].copy()\n",
    "    tmp['pred'] = 0.0\n",
    "    ic = daily_ic(tmp)\n",
    "    mean_ic = ic.mean() if len(ic) else np.nan\n",
    "    mean_ret, std_ret = topk_backtest(tmp, k=200)\n",
    "    print(f\"{split_name} — Baseline0: mean IC={mean_ic:.4f}, TopK mean ret={mean_ret:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline-1: predict previous-day Target per security (where available)\n",
    "val_b1 = val_s.copy()\n",
    "test_b1 = test_s.copy()\n",
    "for frame in [val_b1, test_b1]:\n",
    "    frame['pred'] = frame.groupby('SecuritiesCode')['Target'].shift(1)  # strictly past\n",
    "    frame['pred'] = frame['pred'].fillna(0.0)\n",
    "\n",
    "for name, fr in [('VAL', val_b1), ('TEST', test_b1)]:\n",
    "    ic = (lambda f: (lambda s: s.mean() if len(s) else np.nan)(\n",
    "        (lambda tmp: daily_ic(tmp))(f[['Date','SecuritiesCode','Target','pred']])\n",
    "    ))(fr)\n",
    "    mean_ret, std_ret = topk_backtest(fr[['Date','SecuritiesCode','Target','pred']], k=200)\n",
    "    print(f\"{name} — Baseline1(prev Target): mean IC={ic:.4f}, TopK mean ret={mean_ret:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed112d15",
   "metadata": {},
   "source": [
    "## 6. Models: Simple → Complex (No Leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "def train_eval(model, train_df, val_df, feat_cols):\n",
    "    X_tr = train_df[feat_cols].values\n",
    "    y_tr = train_df['Target'].values\n",
    "    X_va = val_df[feat_cols].values\n",
    "    y_va = val_df['Target'].values\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred_va = model.predict(X_va)\n",
    "    \n",
    "    mae = mean_absolute_error(y_va, pred_va)\n",
    "    r2  = r2_score(y_va, pred_va)\n",
    "    \n",
    "    tmp = val_df[['Date','SecuritiesCode','Target']].copy()\n",
    "    tmp['pred'] = pred_va\n",
    "    ic = daily_ic(tmp).mean()\n",
    "    topk_mean, topk_std = topk_backtest(tmp, k=200)\n",
    "    return {'mae': mae, 'r2': r2, 'mean_ic': ic, 'topk_mean_ret': topk_mean, 'model': model}\n",
    "\n",
    "# Feature set (drop columns with too many NaNs after shifts)\n",
    "nan_ratio = train_s[feat_cols].isna().mean().sort_values()\n",
    "safe_feats = nan_ratio[nan_ratio < 0.2].index.tolist()\n",
    "train_s2 = train_s.dropna(subset=safe_feats + ['Target'])\n",
    "val_s2   = val_s.dropna(subset=safe_feats + ['Target'])\n",
    "test_s2  = test_s.dropna(subset=safe_feats + ['Target'])\n",
    "\n",
    "results = {}\n",
    "\n",
    "# OLS\n",
    "results['OLS'] = train_eval(LinearRegression(), train_s2, val_s2, safe_feats)\n",
    "\n",
    "# Ridge\n",
    "results['Ridge'] = train_eval(Ridge(alpha=1.0, random_state=0), train_s2, val_s2, safe_feats)\n",
    "\n",
    "# Lasso\n",
    "results['Lasso'] = train_eval(Lasso(alpha=1e-4, random_state=0, max_iter=2000), train_s2, val_s2, safe_feats)\n",
    "\n",
    "# Gradient Boosting (interpretable enough with gain-based importance)\n",
    "results['HGB'] = train_eval(HistGradientBoostingRegressor(max_depth=6, learning_rate=0.05, random_state=0), \n",
    "                            train_s2, val_s2, safe_feats)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675bad98",
   "metadata": {},
   "source": [
    "## 7. Pick Best on Validation, Evaluate on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9463eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select by mean IC primarily (rank skill), then topK mean return, then MAE/R2\n",
    "rank = sorted(results.items(), key=lambda kv: (-(kv[1]['mean_ic'] if kv[1]['mean_ic'] is not None else -1),\n",
    "                                               -(kv[1]['topk_mean_ret'] if kv[1]['topk_mean_ret'] is not None else -1)))\n",
    "best_name, best = rank[0]\n",
    "best_name, {k: v for k, v in best.items() if k != 'model'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d23b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Refit best on Train+Val, evaluate on Test\n",
    "best_model = best['model'].__class__(**getattr(best['model'], 'get_params', lambda: {})())\n",
    "X_tv = pd.concat([train_s2, val_s2], axis=0)\n",
    "X_test = test_s2.copy()\n",
    "\n",
    "best_model.fit(X_tv[safe_feats].values, X_tv['Target'].values)\n",
    "pred_te = best_model.predict(X_test[safe_feats].values)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mae = mean_absolute_error(X_test['Target'].values, pred_te)\n",
    "rmse = mean_squared_error(X_test['Target'].values, pred_te, squared=False)\n",
    "r2  = r2_score(X_test['Target'].values, pred_te)\n",
    "\n",
    "tmp = X_test[['Date','SecuritiesCode','Target']].copy()\n",
    "tmp['pred'] = pred_te\n",
    "from scipy.stats import spearmanr\n",
    "ic_series = []\n",
    "for d, g in tmp.groupby('Date'):\n",
    "    if g['pred'].nunique() < 2 or g['Target'].nunique() < 2: \n",
    "        continue\n",
    "    ic_series.append(spearmanr(g['pred'], g['Target']).correlation)\n",
    "ic_series = pd.Series(ic_series).dropna()\n",
    "mean_ic = ic_series.mean() if len(ic_series) else np.nan\n",
    "\n",
    "topk_mean, topk_std = (lambda g: (np.nan, np.nan) if len(g)==0 else (g.mean(), g.std(ddof=1)))(\n",
    "    tmp.groupby('Date').apply(lambda g: g.sort_values('pred', ascending=False).head(200)['Target'].mean())\n",
    ")\n",
    "\n",
    "print({\n",
    "    'model': best_name,\n",
    "    'TEST_MAE': float(mae),\n",
    "    'TEST_RMSE': float(rmse),\n",
    "    'TEST_R2': float(r2),\n",
    "    'TEST_mean_IC': float(mean_ic) if not np.isnan(mean_ic) else None,\n",
    "    'TEST_topK_mean_ret': float(topk_mean) if not np.isnan(topk_mean) else None\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e41f57",
   "metadata": {},
   "source": [
    "## 8. Feature Importance (if supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imp = None\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    imp = pd.Series(best_model.feature_importances_, index=safe_feats).sort_values(ascending=False).head(30)\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    coef = getattr(best_model, 'coef_', None)\n",
    "    if coef is not None and len(np.array(coef).shape) == 1:\n",
    "        imp = pd.Series(coef, index=safe_feats).sort_values(key=lambda x: np.abs(x), ascending=False).head(30)\n",
    "imp.head(20) if imp is not None else \"Model doesn't expose importances\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa7cb0",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Oral Report (3–5 minutes) — Speaking Outline\n",
    "\n",
    "- **Goal:** Predict cross‑sectional `Target` to rank stocks daily; no lookahead.\n",
    "- **Data audit:** Checked shapes/dates/missingness/types; fixed object→numeric; forward/back‑filled O/H/L/C per security in time order.\n",
    "- **Features:** Simple, interpretable: daily range, mid‑price, lagged returns/vol volatility; all shifted to use only past info.\n",
    "- **Split:** Time‑based Train/Val/Test; scaling per security fit on Train only; applied to Val/Test and clipped.\n",
    "- **Baselines:** Predict 0 and previous Target; report IC/Top‑K; sanity‑check uplift.\n",
    "- **Models:** OLS → Ridge/Lasso → Gradient Boosting; choose by **mean IC** and Top‑K; discuss interpretability and stability.\n",
    "- **Results:** (Insert numbers) Best model improved IC from baseline; validate on test; show importances.\n",
    "- **Risks:** Regime shifts, data drift, turnover/transaction costs; monitoring with IC/ICIR.\n",
    "- **Next:** Add sector/size controls, exposure neutralization, and proper portfolio backtest with costs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
